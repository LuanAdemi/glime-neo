from abc import abstractmethod
from concurrent.futures import ThreadPoolExecutor
from typing import Any, Final, List, override
from samplers import AbstractSampler
import torch
from torch.utils.data import DataLoader
import numpy as np
from tqdm import tqdm
import seaborn as sns

import pickle as pkl

from sklearn.decomposition import PCA

import matplotlib.pyplot as plt

class AbstractMetric:
    """
    Abstract class for a metric.

    A metric object evaluates a sampler and returns a result. The result can be anything, but it should be
    a single object that can be used to compare different samplers. For example, a metric could return a list of z-scores
    for a sampler. The metric should be able to render itself by for example a plot.

    The call order of the methods is as follows:
    evaluate() -> render() and result
    """
    def __init__(self, instance: np.ndarray, sampler: AbstractSampler, num_samples: int):
        self.instance = instance
        self.sampler = sampler
        self.num_samples = num_samples

    @abstractmethod
    def render(self, save_path: str = None) -> None:
        pass

    @abstractmethod
    def evaluate(self) -> None:
        pass

    def save(self, root: str, name: str) -> None:
        assert self.result is not None, "Result not calculated yet."
        assert root.endswith("/"), "Root should end with a /"
        assert not "/" in name, "Name should not contain /"

        self.render(root + name + "_plot.png")
        with open(root + name + "_results.pkl", "wb") as f:
            pkl.dump(self.result, f)

    @property
    def result(self) -> Any:
        pass


class ZScoreMetric(AbstractMetric):
    """
    Metric that calculates the z-scores of the samples generated by a sampler. It captures "how far away" the samples are from the
    training dataset distribution.

    We first calculate the mean and standard deviation of the training dataset, and then calculate the z-scores of the samples generated
    by the sampler. The z-scores are calculated as (x - mu) / sigma, where x is the sample, mu is the mean of the training dataset and
    sigma is the standard deviation of the training dataset.
    """
    def __init__(self, instance: np.ndarray, sampler: AbstractSampler, num_samples: int, dataset: DataLoader):
        super().__init__(instance, sampler, num_samples)
        self.dataset = dataset
        self.mean = None
        self.std = None
        self.z_scores = None

    def _calculate_mean_and_std(self, progress=False):
        assert self.mean is None and self.std is None, "Mean and std already calculated. This call shouldn't happen."
        mean = 0
        std = 0

        # NOTE: this should be multithreaded by default since it uses a dataloader
        for (x, _) in tqdm(self.dataset, desc="Calculating mean and std", disable=not progress):
            mean += x.mean()
            std += x.std()
        mean /= len(self.dataset)
        std /= len(self.dataset)

        self.mean = mean
        self.std = std

    @staticmethod
    def _calculate_z_scores(sample: np.ndarray, mean: float, std: float):
        return (sample[0].mean() - mean) / std

    @override
    def evaluate(self, progress=False):
        self._calculate_mean_and_std(progress)

        z_scores = []

        with ThreadPoolExecutor() as executor:
            futures = []
            for sample in self.sampler.sample(self.num_samples, progress=progress):
                futures.append(executor.submit(self._calculate_z_scores, sample, self.mean, self.std))
            
            for future in tqdm(futures, desc="Calculating z-scores", disable=not progress):
                z_scores.append(future.result())

        self.z_scores = np.array(z_scores)

    @override
    def render(self, save_path: str = None):
        assert self.z_scores is not None, "Z-scores not calculated yet."

        plt.figure()
        sns.histplot(self.z_scores, bins=50)

        plt.xlabel("Z-score")
        plt.ylabel("Frequency")
        plt.title("Z-scores of samples")

        if save_path is not None:
            plt.savefig(save_path)
        else:
            plt.show()

    @override
    @property
    def result(self):
        assert self.z_scores is not None, "Z-scores not calculated yet."
        return self.z_scores

class PredictionVarianceMetric(AbstractMetric):
    def __init__(self, instance: np.ndarray, sampler: AbstractSampler, num_samples: int):
        super().__init__(instance, sampler, num_samples)
        self.predictions = None
        self.variance = None

class SamplerEvaluation:
    """
    Class to evaluate a sampler.

    Note that that an evaluation object is tied to a specific evaluation run, so all attributes are declared as final.
    """
    def __init__(self, sampler: AbstractSampler, num_samples: int, metrics: List[AbstractMetric]):
        self.sampler: Final = sampler
        self.num_samples: Final = num_samples
        self.metrics: Final = metrics

    